{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71254553",
      "metadata": {
        "id": "71254553"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.6,max_split_size_mb:128,expandable_segments:True\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14913f8d",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "14913f8d",
        "outputId": "888f4858-8888-4c65-abf8-9ef9cccebdae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m287.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m222.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=4c4a07ee96cfcc903a9f5b19f603e9f18e450c7e59505e7e8bddeb7401b35876\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r_tz0q2k/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: pyarrow, rouge_score, datasets, bitsandbytes, evaluate\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.48.2 datasets-4.3.0 evaluate-0.4.6 pyarrow-22.0.0 rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "%pip install --no-cache-dir -U transformers peft accelerate datasets evaluate bitsandbytes rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f13529",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "47f13529",
        "outputId": "f5c8e281-498e-4466-b4c0-49cae79fbdf5"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/3a2mext/3A2M_EXTENDED.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2789710624.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/3a2mext/3A2M_EXTENDED.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrecipes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {len(recipes)} recipes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/3a2mext/3A2M_EXTENDED.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_path = \"/kaggle/input/3a2mext/3A2M_EXTENDED.csv\"\n",
        "recipes = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"Loaded {len(recipes)} recipes.\")\n",
        "\n",
        "recipes.dropna(subset=['title'], inplace=True)\n",
        "\n",
        "recipes['title'] = recipes['title'].str.strip()\n",
        "recipes['title'] = recipes['title'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
        "\n",
        "recipes['genre'] = recipes['genre'].str.lower()\n",
        "recipes['genre'] = recipes['genre'].str.strip()\n",
        "\n",
        "def parse_string_list(text):\n",
        "    try:\n",
        "        return ast.literal_eval(text)\n",
        "    except (ValueError, SyntaxError):\n",
        "        return []\n",
        "\n",
        "list_columns = ['NER', 'Extended_NER', 'directions']\n",
        "\n",
        "print(\"Converting string-based lists\")\n",
        "for col in list_columns:\n",
        "    print(f\"Processing '{col}'...\")\n",
        "    recipes[col] = recipes[col].apply(parse_string_list)\n",
        "\n",
        "print(\"Conversion complete!\")\n",
        "\n",
        "print(\"\\n Preprocessing Done. Here's the new data info: \")\n",
        "recipes.info()\n",
        "\n",
        "print(\"\\n And here's the cleaned data: \")\n",
        "print(recipes.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa362aac",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fa362aac"
      },
      "outputs": [],
      "source": [
        "import random, math, ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
        "\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "rng = np.random.default_rng(random_seed)\n",
        "\n",
        "recipe_book = recipes.copy()\n",
        "\n",
        "def _ensure_list(x):\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    try:\n",
        "        return ast.literal_eval(x) if isinstance(x, str) and x.strip() else []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "for col in (\"NER\", \"directions\"):\n",
        "    if col in recipe_book.columns:\n",
        "        recipe_book[col] = recipe_book[col].apply(_ensure_list)\n",
        "    else:\n",
        "        recipe_book[col] = [[] for _ in range(len(recipe_book))]\n",
        "\n",
        "mask_has_content = recipe_book['NER'].map(bool) & recipe_book['directions'].map(bool)\n",
        "recipe_book = recipe_book[mask_has_content].reset_index(drop=True)\n",
        "\n",
        "def build_recipe_string_for_length(row):\n",
        "    title = (row.get('title') or \"\").strip()\n",
        "    ingredients = row.get('NER') or []\n",
        "    directions = row.get('directions') or []\n",
        "    ing_text = \"; \".join([str(i).strip() for i in ingredients if str(i).strip()])\n",
        "    dir_text = \" \".join([str(s).strip() for s in directions if str(s).strip()])\n",
        "    return f\"TITLE: {title}\\nINGREDIENTS: {ing_text}\\nDIRECTIONS: {dir_text}\\n<|endoftext|>\"\n",
        "\n",
        "texts_for_len = recipe_book.apply(build_recipe_string_for_length, axis=1).tolist()\n",
        "char_lengths = np.array([len(t) for t in texts_for_len], dtype=int)\n",
        "length_stats_full = {\n",
        "    \"count\": int(len(char_lengths)),\n",
        "    \"min\": int(char_lengths.min()) if len(char_lengths) else 0,\n",
        "    \"25%\": int(np.percentile(char_lengths,25)) if len(char_lengths) else 0,\n",
        "    \"median\": int(np.median(char_lengths)) if len(char_lengths) else 0,\n",
        "    \"75%\": int(np.percentile(char_lengths,75)) if len(char_lengths) else 0,\n",
        "    \"max\": int(char_lengths.max()) if len(char_lengths) else 0,\n",
        "    \"mean\": float(char_lengths.mean()) if len(char_lengths) else 0.0\n",
        "}\n",
        "\n",
        "desired_sample_size = 30_000\n",
        "N = len(texts_for_len)\n",
        "if desired_sample_size >= N:\n",
        "    sample_indices = list(range(N))\n",
        "else:\n",
        "    n_bins = 20\n",
        "    quantiles = np.linspace(0,100,n_bins+1)\n",
        "    bin_edges = np.percentile(char_lengths, quantiles)\n",
        "    bin_ids = np.digitize(char_lengths, bins=bin_edges[1:-1], right=True)\n",
        "    indices_by_bin = [np.where(bin_ids==i)[0].tolist() for i in range(n_bins)]\n",
        "    counts = np.array([len(idx) for idx in indices_by_bin], dtype=int)\n",
        "    proportions = counts / counts.sum()\n",
        "    raw_alloc = proportions * desired_sample_size\n",
        "    alloc = np.floor(raw_alloc).astype(int)\n",
        "    alloc = np.minimum(alloc, counts)\n",
        "    current = alloc.sum()\n",
        "    remainder = desired_sample_size - current\n",
        "    if remainder > 0:\n",
        "        remainders = raw_alloc - np.floor(raw_alloc)\n",
        "        order = np.argsort(remainders)[::-1]\n",
        "        for i in order:\n",
        "            if remainder <= 0:\n",
        "                break\n",
        "            if alloc[i] < counts[i]:\n",
        "                alloc[i] += 1\n",
        "                remainder -= 1\n",
        "    elif remainder < 0:\n",
        "        deficit = -remainder\n",
        "        order = np.argsort(raw_alloc - alloc)\n",
        "        for i in order:\n",
        "            if deficit <= 0:\n",
        "                break\n",
        "            remove = min(deficit, alloc[i])\n",
        "            alloc[i] -= remove\n",
        "            deficit -= remove\n",
        "    for i in range(n_bins):\n",
        "        if alloc[i] > counts[i]:\n",
        "            alloc[i] = counts[i]\n",
        "    sample_indices = []\n",
        "    for i in range(n_bins):\n",
        "        if alloc[i] <= 0:\n",
        "            continue\n",
        "        chosen = rng.choice(indices_by_bin[i], size=alloc[i], replace=False).tolist()\n",
        "        sample_indices.extend(chosen)\n",
        "    sampled_set = set(sample_indices)\n",
        "    if len(sample_indices) < desired_sample_size:\n",
        "        remaining_needed = desired_sample_size - len(sample_indices)\n",
        "        all_remaining = [i for i in range(N) if i not in sampled_set]\n",
        "        if len(all_remaining) <= remaining_needed:\n",
        "            sample_indices.extend(all_remaining)\n",
        "        else:\n",
        "            extra = rng.choice(all_remaining, size=remaining_needed, replace=False).tolist()\n",
        "            sample_indices.extend(extra)\n",
        "    if len(sample_indices) > desired_sample_size:\n",
        "        sample_indices = rng.choice(sample_indices, size=desired_sample_size, replace=False).tolist()\n",
        "    rng.shuffle(sample_indices)\n",
        "\n",
        "sampled_texts = [texts_for_len[i] for i in sample_indices]\n",
        "hf_df = pd.DataFrame({\"text\": sampled_texts})\n",
        "hf_ds = Dataset.from_pandas(hf_df)\n",
        "\n",
        "try:\n",
        "    word_packer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True, local_files_only=True)\n",
        "except Exception:\n",
        "    try:\n",
        "        from transformers import GPT2TokenizerFast\n",
        "        word_packer = GPT2TokenizerFast.from_pretrained(\"gpt2\", local_files_only=True)\n",
        "    except Exception:\n",
        "        word_packer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
        "\n",
        "if word_packer.pad_token is None:\n",
        "    word_packer.pad_token = word_packer.eos_token\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return word_packer(batch[\"text\"], truncation=True, max_length=192, padding=False)\n",
        "\n",
        "tokenized_recipes = hf_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "lengths_tokens = np.array([len(ids) for ids in tokenized_recipes[\"input_ids\"]], dtype=int)\n",
        "token_length_stats_sampled = {\n",
        "    \"count\": int(len(lengths_tokens)),\n",
        "    \"min\": int(lengths_tokens.min()) if len(lengths_tokens) else 0,\n",
        "    \"25%\": int(np.percentile(lengths_tokens,25)) if len(lengths_tokens) else 0,\n",
        "    \"median\": int(np.median(lengths_tokens)) if len(lengths_tokens) else 0,\n",
        "    \"75%\": int(np.percentile(lengths_tokens,75)) if len(lengths_tokens) else 0,\n",
        "    \"max\": int(lengths_tokens.max()) if len(lengths_tokens) else 0,\n",
        "    \"mean\": float(lengths_tokens.mean()) if len(lengths_tokens) else 0.0\n",
        "}\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(char_lengths, bins=40, edgecolor='black')\n",
        "plt.title(\"Character Length Distribution (full)\")\n",
        "plt.xlabel(\"Characters per Recipe\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(lengths_tokens, bins=40, edgecolor='black')\n",
        "plt.title(\"Token Length Distribution (sampled)\")\n",
        "plt.xlabel(\"Tokens per Recipe\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "save_path = \"/kaggle/working/tokenized_recipes\"\n",
        "tokenized_recipes.save_to_disk(save_path)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=word_packer, mlm=False)\n",
        "\n",
        "print(f\"total_recipes: {len(recipe_book)}\")\n",
        "print(f\"sampled_dataset_size: {len(sampled_texts)}\")\n",
        "print(f\"tokenized_sampled_size: {len(tokenized_recipes)}\")\n",
        "print(f\"char_length_stats_full: {length_stats_full}\")\n",
        "print(f\"token_length_stats_sampled: {token_length_stats_sampled}\")\n",
        "print(f\"RECOMMENDED MAX_LENGTH (75th percentile): {token_length_stats_sampled['75%']}\")\n",
        "print(f\"tokenizer_vocab_size: {len(word_packer)}\")\n",
        "print(f\"example_formatted_recipe_first_1000_chars:\\n{sampled_texts[0][:1000]}\\n\")\n",
        "print(f\"example_first_input_ids_sampled (first 40 tokens):\\n{tokenized_recipes['input_ids'][0][:40]}\")\n",
        "print(f\"saved_tokenized_dataset_to: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f6b2f4",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "e8259cd0504443ef9f1b8943768a1ee1",
            "ccd966c6fb8b433b823be7c7582b0a82"
          ]
        },
        "id": "80f6b2f4",
        "outputId": "110b37fe-a223-4d7b-b4ac-e7985bce4c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset splits: Train=18000, Val=6000, Test=6000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8259cd0504443ef9f1b8943768a1ee1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccd966c6fb8b433b823be7c7582b0a82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model params total: 83,152,128 — trainable: 1,179,648 (1.4187%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1125/1125 49:54, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.611700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.479200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 01:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 2.3295 -> Perplexity: 10.27\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='263' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 263/1125 11:44 < 38:46, 0.37 it/s, Epoch 0.23/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os, random, math, torch, gc\n",
        "from datasets import load_from_disk\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "TEST_RUN_SAMPLE_SIZE = None\n",
        "\n",
        "tokenized_candidates = [\"/kaggle/working/tokenized_recipes\", \"tokenized_recipes\"]\n",
        "tokenized_path = next((p for p in tokenized_candidates if os.path.exists(p)), None)\n",
        "if tokenized_path is None:\n",
        "    raise FileNotFoundError(f\"Tokenized dataset not found in {tokenized_candidates}. Run tokenization cell first.\")\n",
        "\n",
        "dataset = load_from_disk(tokenized_path)\n",
        "\n",
        "train_val_test = dataset.train_test_split(test_size=0.2, seed=random_seed)\n",
        "train_val = train_val_test[\"train\"]\n",
        "test_dataset = train_val_test[\"test\"]\n",
        "\n",
        "train_val_split = train_val.train_test_split(test_size=0.25, seed=random_seed)\n",
        "train_dataset = train_val_split[\"train\"]\n",
        "eval_dataset = train_val_split[\"test\"]\n",
        "\n",
        "del dataset, train_val_test, train_val, train_val_split\n",
        "gc.collect()\n",
        "\n",
        "if TEST_RUN_SAMPLE_SIZE:\n",
        "    print(f\"CONDUCTING TEST RUN WITH SAMPLE SIZE: {TEST_RUN_SAMPLE_SIZE} \")\n",
        "    train_dataset = train_dataset.shuffle(seed=random_seed).select(range(min(TEST_RUN_SAMPLE_SIZE, len(train_dataset))))\n",
        "    eval_dataset = eval_dataset.shuffle(seed=random_seed).select(range(min(TEST_RUN_SAMPLE_SIZE // 4, len(eval_dataset))))\n",
        "    test_dataset = test_dataset.shuffle(seed=random_seed).select(range(min(TEST_RUN_SAMPLE_SIZE // 4, len(test_dataset))))\n",
        "\n",
        "print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(eval_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "try:\n",
        "    word_packer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True, local_files_only=True)\n",
        "except Exception:\n",
        "    try:\n",
        "        word_packer = GPT2TokenizerFast.from_pretrained(\"gpt2\", local_files_only=True)\n",
        "    except Exception:\n",
        "        word_packer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
        "\n",
        "if word_packer.pad_token is None:\n",
        "    word_packer.pad_token = word_packer.eos_token\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=word_packer, mlm=False)\n",
        "\n",
        "NUM_EPOCHS = 2\n",
        "LORA_R = 8\n",
        "PER_DEVICE_BATCH = 1\n",
        "GRAD_ACCUM = 16\n",
        "EVAL_STEPS = 500\n",
        "DATALOADER_WORKERS = 0\n",
        "\n",
        "compute_dtype = torch.float16 if torch.cuda.is_available() else torch.bfloat16\n",
        "\n",
        "memory_saver_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=compute_dtype\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device_index = torch.cuda.current_device()\n",
        "    kitchen_gpt = AutoModelForCausalLM.from_pretrained(\"gpt2\", device_map={\"\": device_index}, quantization_config=memory_saver_config)\n",
        "else:\n",
        "    kitchen_gpt = AutoModelForCausalLM.from_pretrained(\"gpt2\", device_map=\"auto\", quantization_config=memory_saver_config)\n",
        "\n",
        "kitchen_gpt.config.use_cache = False\n",
        "kitchen_gpt.resize_token_embeddings(len(word_packer))\n",
        "kitchen_gpt = prepare_model_for_kbit_training(kitchen_gpt)\n",
        "\n",
        "adapter_config = LoraConfig(r=LORA_R, lora_alpha=max(1, LORA_R * 2), target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
        "kitchen_gpt = get_peft_model(kitchen_gpt, adapter_config)\n",
        "\n",
        "def count_params(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "_total, _trainable = count_params(kitchen_gpt)\n",
        "print(f\"model params total: {_total:,} — trainable: {_trainable:,} ({_trainable/_total:.4%})\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "training_plan = TrainingArguments(\n",
        "    output_dir=\"/kaggle/working/kitchen_gpt_ckpt\",\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    fp16=True if torch.cuda.is_available() else False,\n",
        "    gradient_checkpointing=True,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    dataloader_num_workers=DATALOADER_WORKERS,\n",
        "    report_to=\"none\",\n",
        "    logging_steps=EVAL_STEPS,\n",
        "    save_strategy=\"no\",\n",
        "    eval_accumulation_steps=1,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "def compute_metrics(eval_pred):\n",
        "    if isinstance(eval_pred, (tuple, list)):\n",
        "        logits, labels = eval_pred\n",
        "    else:\n",
        "        logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
        "    if logits is None:\n",
        "        return {\"accuracy\": 0.0}\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    mask = labels != -100\n",
        "    if mask.sum() == 0:\n",
        "        return {\"accuracy\": 0.0}\n",
        "    correct = (preds == labels) & mask\n",
        "    acc = correct.sum() / mask.sum()\n",
        "    return {\"accuracy\": float(acc)}\n",
        "\n",
        "main_trainer = Trainer(\n",
        "    model=kitchen_gpt,\n",
        "    args=training_plan,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_result = main_trainer.train()\n",
        "    del train_result\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    main_trainer.save_state()\n",
        "    kitchen_gpt.save_pretrained(f\"/kaggle/working/kitchen_gpt_lora_epoch{epoch+1}\")\n",
        "\n",
        "    eval_res = main_trainer.evaluate()\n",
        "    eval_loss = eval_res.get(\"eval_loss\") or eval_res.get(\"loss\")\n",
        "    if eval_loss is not None:\n",
        "        try:\n",
        "            ppl = math.exp(eval_loss)\n",
        "        except Exception:\n",
        "            ppl = float(\"inf\")\n",
        "        print(f\"Validation loss: {eval_loss:.4f} -> Perplexity: {ppl:.2f}\")\n",
        "    else:\n",
        "        print(\"No validation loss returned\")\n",
        "\n",
        "    del eval_res\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "test_pred = main_trainer.predict(test_dataset)\n",
        "test_loss = test_pred.metrics.get(\"test_loss\") or test_pred.metrics.get(\"eval_loss\") or test_pred.metrics.get(\"loss\")\n",
        "if test_loss is not None:\n",
        "    try:\n",
        "        test_ppl = math.exp(test_loss)\n",
        "    except Exception:\n",
        "        test_ppl = float(\"inf\")\n",
        "    print(f\"Test loss: {test_loss:.4f} -> Perplexity: {test_ppl:.2f}\")\n",
        "else:\n",
        "    print(\"No test loss returned\")\n",
        "\n",
        "del test_pred\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "try:\n",
        "    import evaluate\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    small_test_set = test_dataset.shuffle(seed=random_seed).select(range(min(200, len(test_dataset))))\n",
        "    predictions = []\n",
        "    references = []\n",
        "    device = next(kitchen_gpt.parameters()).device\n",
        "\n",
        "    for ex in small_test_set:\n",
        "        if \"text\" in ex:\n",
        "            raw = ex[\"text\"]\n",
        "        else:\n",
        "            raw = word_packer.decode(ex[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "        if \"DIRECTIONS:\" in raw:\n",
        "            prompt = raw.split(\"DIRECTIONS:\")[0] + \"DIRECTIONS:\"\n",
        "            ref = raw.split(\"DIRECTIONS:\")[-1].replace(\"<|endoftext|>\", \"\").strip()\n",
        "        else:\n",
        "            prompt = raw[:400]\n",
        "            ref = \"\"\n",
        "\n",
        "        inputs = word_packer(prompt, return_tensors=\"pt\").to(device)\n",
        "        out = kitchen_gpt.generate(**inputs, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, max_new_tokens=200, pad_token_id=word_packer.eos_token_id)\n",
        "        gen = word_packer.decode(out[0], skip_special_tokens=True)\n",
        "        pred = gen.replace(prompt, \"\").strip()\n",
        "        predictions.append(pred)\n",
        "        references.append(ref)\n",
        "\n",
        "    rouge_res = rouge.compute(predictions=predictions, references=references)\n",
        "    bleu_res = bleu.compute(predictions=[p.split() for p in predictions], references=[[r.split()] for r in references])\n",
        "    print(\"ROUGE (test set):\", {k: v for k, v in rouge_res.items()})\n",
        "    print(\"BLEU (test set):\", bleu_res)\n",
        "\n",
        "    del rouge, bleu, small_test_set, predictions, references, inputs, out, gen, rouge_res, bleu_res\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Skipping ROUGE/BLEU/generation metrics due to error: {e}\")\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "kitchen_gpt.eval()\n",
        "prompts = [\n",
        "    \"TITLE: Cheesy Garlic Bread\\nINGREDIENTS:\",\n",
        "    \"TITLE: Quick Dinner\\nINGREDIENTS: ['ground beef', 'onion', 'canned tomatoes', 'kidney beans']\\nDIRECTIONS:\"\n",
        "]\n",
        "device = next(kitchen_gpt.parameters()).device\n",
        "for p in prompts:\n",
        "    inputs = word_packer(p, return_tensors=\"pt\").to(device)\n",
        "    out = kitchen_gpt.generate(**inputs, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, max_new_tokens=350, pad_token_id=word_packer.eos_token_id)\n",
        "    text = word_packer.decode(out[0], skip_special_tokens=True)\n",
        "    print(\"\\n GENERATED (truncated) \\n\", text[:1500])\n",
        "\n",
        "del kitchen_gpt, main_trainer, word_packer, data_collator, train_dataset, eval_dataset, test_dataset\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 2957522,
          "sourceId": 5093016,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31155,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6513.430094,
      "end_time": "2025-11-02T18:46:15.334030",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-11-02T16:57:41.903936",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}